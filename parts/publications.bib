

@article{guedes_extending_2017,
  title        = {Extending multimedia languages to support multimodal user interactions},
  volume       = {76},
  rights       = {All rights reserved},
  issn         = {1573-7721},
  url          = {https://doi.org/10.1007/s11042-016-3846-8},
  doi          = {10.1007/s11042-016-3846-8},
  abstract     = {Historically, the Multimedia community research has focused on output modalities, through studies on timing and multimedia processing. The Multimodal Interaction community, on the other hand, has focused on user-generated modalities, through studies on Multimodal User Interfaces ({MUI}). In this paper, aiming to assist the development of multimedia applications with {MUIs}, we propose the integration of concepts from those two communities in a unique high-level programming framework. The framework integrates user modalities —both user-generated (e.g., speech, gestures) and user-consumed (e.g., audiovisual, haptic)— in declarative programming languages for the specification of interactive multimedia applications. To illustrate our approach, we instantiate the framework in the {NCL} (Nested Context Language) multimedia language. {NCL} is the declarative language for developing interactive applications for Brazilian Digital {TV} and an {ITU}-T Recommendation for {IPTV} services. To help evaluate our approach, we discuss a usage scenario and implement it as an {NCL} application extended with the proposed multimodal features. Also, we compare the expressiveness of the multimodal {NCL} against existing multimedia and multimodal languages, for both input and output modalities.},
  pages        = {5691--5720},
  number       = {4},
  journaltitle = {Multimedia Tools and Applications},
  shortjournal = {Multimed Tools Appl},
  author       = {Guedes, Alan Livio Vasconcelos and Azevedo, Roberto Gerson de Albuquerque and Barbosa, Simone Diniz Junqueira},
  year = {2017},
  langid       = {english}
}

@article{pereira_ontology-based_2020,
  title        = {An ontology-based approach to integrate {TV} and {IoT} middlewares},
  rights       = {2020 Springer Science+Business Media, {LLC}, part of Springer Nature},
  issn         = {1573-7721},
  url          = {https://link.springer.com/article/10.1007/s11042-020-09645-4},
  doi          = {10.1007/s11042-020-09645-4},
  abstract     = {Internet of Things ({IoT}) is the interconnection of physical devices, known of smart objects, that share data and collaborate among them to support users. There are {IoT} usage scenarios in several domains. In-home domain, the use of smart objects may support monitoring and automation of the environment. In a home, the {TV} is one of the most used devices. It may also collaborate with the smart objects to enhance the interactive experience of viewers on watching. To support such a meeting, we propose an (i) conceptual model, {IoTTV}-Ont, that allows modeling entire homes, with smart objects, people' profile, and viewers' interaction events with {TV} applications; (ii) a software architecture that integrates and interoperates digital {TV} and {IoT} middleware. This architecture allows the {TV} applications: to be aware of the physical environment context, to change aspects of the physical environment, to identify the viewers, and to receive multimodal interactions performed by viewers. We demonstrate our work through use cases. For this, we implement a prototype of our software architecture and modeled home using {IoTTV}-Ont. Then we developed usage scenarios that enhance {TV} viewers' experience by supporting features such as content adaptation, multi-sensorial immersive experience, and multimodal user interaction.},
  pages        = {1--25},
  journaltitle = {Multimedia Tools and Applications},
  shortjournal = {Multimed Tools Appl},
  author       = {Pereira, Danne Makleyston G. and Silva, Francisco José da S. e and Neto, Carlos de Salles S. and Santos, Davi Viana dos and Coutinho, Luciano Reis and Guedes, Alan L. V.},
  year = {2020},
  langid       = {english}
}
@incollection{colcher_current_2017,
	title = {Current Projects and Future Vision of the {TeleMídia} / {PUC}-Rio Laboratory in Videocolaboration},
	rights = {All rights reserved},
	isbn = {978-85-7669-381-9},
	url = {http://www.inf.ufrgs.br/webmedia2017/wp-content/livro_worksho_ctvideo.pdf},
	abstract = {Since the early 1990's, the {TeleMídia} laboratory of the Department of Informatics of {PUC}-Rio researches and trains people in the areas of multimedia and hypermedia, which include interactive videos and video collaboration as important subareas. This chapter presents the main research lines and projects of the laboratory and discusses the future vision of the authors about interactive videos and video collaboration.},
	pages = {207--323},
	booktitle = {Anais do {XXIII} Simpósio Brasileiro de Sistemas Multimídia e Web: Workshop do {CT}-Vídeo (Comitê Técnico de Prospecção Tecnológica em Videocolaboração)},
	author = {Colcher, Sérgio and Guedes, Alan L. V. and Azevedo, Roberto Gerson de A. and Lima, Guilherme F. and Santos, Rodrigo C. M. and Busson, Antonio J. G.},
	year = {2017}
}

@incollection{roriz_junior_introduction_2019,
	edition = {1},
	title = {Introduction to Data Flow Processing: A Complex Event-Oriented Approach},
	rights = {All rights reserved},
	isbn = {978-85-7669-481-6},
	url = {https://doi.org/10.5753/sbc.481.6.02},
	shorttitle = {Introdução ao processamento de ﬂuxo de dados},
	pages = {45--76},
	booktitle = {Minicursos do {XXV} Simpósio Brasileiro de Sistemas Multimídia e Web},
	publisher = {{SBC}},
	author = {Roriz Junior, Marcos and {Alan L. V. Guedes} and {Fernando B. V. Magalhães} and {Sérgio Colcher} and {Markus Endler}},
	year = {2019}
}

@incollection{antonio_jose_g_busson_embedding_2020,
	location = {Cham},
	title = {Embedding Deep Learning Models into Hypermedia Applications},
	isbn = {978-3-030-35102-1},
	url = {https://doi.org/10.1007/978-3-030-35102-1_4},
	abstract = {Deep learning research has allowed significant advances in several areas of multimedia, especially in tasks related to speech processing, hearing, and computational vision. Particularly, recent usage scenarios in hypermedia domain already use such deep learning tasks to build applications that are sensitive to its media content semantics. However, the development of such scenarios is usually done from scratch. In particular, current hypermedia standards such as {HTML} do not fully support such kind of development. To support such development, we propose that a hypermedia language should be extended to support: (1) describe learning using structured media datasets; (2) recognize content semantics of the media elements in presentation time; (3) use the recognized semantics elements as events in during the multimedia. To illustrate our approach, we extended the {NCL} language, and its model {NCM}, to support such features. {NCL} (Nested Context Language) is the declarative language for developing interactive applications for Brazilian Digital {TV} and an {ITU}-T Recommendation for {IPTV} services. As a result of the work, it is presented a usage scenario to highlight how the extended {NCL} supports the development of content-aware hypermedia presentations, attesting the expressiveness and applicability of the model.},
	pages = {91--111},
	booktitle = {Special Topics in Multimedia, {IoT} and  Web Technologies},
	publisher = {Springer International Publishing},
	author = {{Antonio José G. Busson} and {Alan Livio V. Guedes} and {Sérgio Colcher} and {Ruy Luiz Milidiú} and {Edward Hermann Haeusler}},
	editor = {Roesler, Valter and Barrére, Eduardo and Willrich, Roberto},
	year = {2020},
	langid = {english}
}

@incollection{kulesza_ginga-j_2011,
	location = {Berlin, Heidelberg},
	title = {Ginga-J - An Open Java-Based Application Environment for Interactive Digital Television Services},
	rights = {All rights reserved},
	isbn = {978-3-642-24418-6},
	url = {https://doi.org/10.1007/978-3-642-24418-6_3},
	series = {{IFIP} Advances in Information and Communication Technology},
	pages = {34--49},
	booktitle = {Open Source Systems: Grounding Research},
	publisher = {Springer},
	author = {Kulesza, Raoni and Lima, Jefferson F. A. and Guedes, Alan L. and Junior, Lucenildo L. A. and Meira, Silvio R. L. and Filho, Guido L. S.},
	editor = {Hissam, Scott A. and Russo, Barbara and de Mendonça Neto, Manoel G. and Kon, Fabio},
	year = {2011},
	langid = {english}
}

@incollection{dos_santos_deep_2019,
	edition = {1},
	title = {Deep Learning Based Methods for Video Analysis},
	rights = {All rights reserved},
	isbn = {978-85-7669-481-6},
	url = {https://doi.org/10.5753/sbc.481.6.04},
	pages = {119--166},
	booktitle = {Minicursos Do {XXV} Simpósio Brasileiro de Sistemas Multimídia e Web},
	publisher = {{SBC}},
	author = {dos Santos, Gabriel N. P. and de Freitas, Pedro V. A. and Busson, Antonio José G. and Guedes, Alan L. V. and Colcher, Sérgio and Milidiú, Ruy L.},
	year = {2019},
	doi = {10.5753/sbc.481.6.04}
}


@inproceedings{guedes_modeling_2019,
	location = {New York, {NY}, {USA}},
	title = {Modeling Multimodal-Multiuser Interactions in Declarative Multimedia Languages},
	rights = {All rights reserved},
	isbn = {978-1-4503-6887-2},
	url = {http://doi.acm.org/10.1145/3342558.3345400},
	doi = {10.1145/3342558.3345400},
	series = {{DocEng} '19},
	abstract = {Recent advances in hardware and software technologies have given rise to a new class of human-computer interfaces that both explores multiple modalities and allows for multiple collaborating users. When compared to the development of traditional single-user {WIMP} (windows, icons, menus, pointer)-based applications, however, applications supporting the seamless integration of multimodal-multiuser interactions bring new specification and runtime requirements. With the aim of assisting the specification of multimedia applications that integrate multimodal-multiuser interactions, this paper: (1) proposes the {MMAM} (Multimodal-Multiuser Authoring Model); (2) presents three different instantiations of it (in {NCL}, {HTML}, and a block-based syntax); and (3) evaluates the proposed model through a task-based user study. {MMAM} enables programmers to design and ponder different solutions for applications with multimodal-multiuser requirements. The proposed instantiations served as proofs of concept about the feasibility of our model implementation and provided the basis for practical experimentation, while the performed user study focused on capturing evidence of both the user understanding and the user acceptance of the proposed model. We asked developers to perform tasks using {MMAM} and then answer a {TAM} (Technology Acceptance Model)-based questionnaire focused on both the model and its instances. As results, the study indicates that the participants easily understood the model (most of them performed the required tasks with minor or no errors) and found it both useful and easy to use. 94.47\% of the participants gave positive answers to the block-based representation {TAM} questions, whereas 75.17\% gave positive answers to the instances-related questions.},
	pages = {11:1--11:10},
	booktitle = {Proceedings of the {ACM} Symposium on Document Engineering 2019},
	publisher = {{ACM}},
	author = {Guedes, Alan L. V. and de A. Azevedo, Roberto G. and Colcher, Sérgio and Barbosa, Simone D. J.},
	year = {2019}
}

@inproceedings{jesus_cnn-based_2019,
	location = {San Diego, {CA}, {USA}},
	title = {A {CNN}-Based Tool to Index Emotion on Anime Character Stickers},
	rights = {All rights reserved},
	isbn = {978-1-72815-606-4},
	url = {https://ieeexplore.ieee.org/document/8959002/},
	doi = {10.1109/ISM46123.2019.00071},
	eventtitle = {2019 {IEEE} International Symposium on Multimedia ({ISM})},
	pages = {319--3193},
	booktitle = {2019 {IEEE} International Symposium on Multimedia ({ISM})},
	publisher = {{IEEE}},
	author = {Jesus, Ivan and Cardoso, Jessica and Busson, Antonio Jose G. and Guedes, Alan Livio and Colcher, Sergio and Milidiu, Ruy Luiz},
	year = {2019}
}

@inproceedings{dodsworth_dynamic_2018,
	title = {Dynamic Integration of Foreign-Language Parsers into an {NCL} Player},
	rights = {All rights reserved},
	doi = {10.1145/3243082.3243095},
	abstract = {We describe how we modified an {NCL} player to accept as input, in addition to {NCL} documents, Lua scripts. These Lua scripts evaluate to a table in a particular format, called {NCL}-ltab, which is a Lua table encoding of the {NCL} player's internal model. One advantage of our modifications is that they allow the {NCL} parsing to occur in the Lua script, i.e., outside the formatter but integrated in its execution flow. The same applies for parsers of dialects of {NCL} or similar languages. Another advantage is that new parsers can be plugged dynamically into the formatter (if they are written in Lua or can be called from Lua). In this paper, we detail the internal model of the {NCL} player we are using and the {NCL}-ltab input format. To evaluate our proposal, we present two parser-integration use cases, one for {NCL} itself, using the {DietNCL} parsing toolkit, and another for {sNCL}, an alternative, user-friendlier syntax for {NCL}.},
	pages = {8},
	booktitle = {Proceedings of the 24st Brazilian Symposium on Multimedia and the Web},
	author = {Dodsworth, Jorge P and Terças, Lucas de Macêdo and Alan L. V. Guedes and Lima, Guilherme F. and Neto, Carlos de Salles Soares and Colcher, Sérgio},
	year = {2018},
	langid = {english}
}

@inproceedings{de_freitas_ergonomic_2019,
	location = {New York, {NY}, {USA}},
	title = {An Ergonomic Evaluation Method Using a Mobile Depth Sensor and Pose Estimation},
	rights = {All rights reserved},
	isbn = {978-1-4503-6763-9},
	url = {http://doi.acm.org/10.1145/3323503.3349550},
	doi = {10.1145/3323503.3349550},
	series = {{WebMedia} '19},
	abstract = {An ergonomic evaluation is an observation of a person in order to identify musculoskeletal disorders ({WMSDs}) caused by prolonged or repeated harmful poses that a person adopts during work tasks. Nowadays, an ergonomist or other health professional perform such evaluations based on a set of posture rules and checklists, which can be subjective and thus lead to erroneous risk classifications. Moreover, this professional usually perform such evaluation in the patient work environment. In order to make those evaluations more objective and concise we propose a evaluation method using a mobile depth sensor. Different from other methods based in fixed depth sensors (e.g. Kinect), our method enable professionals easily perform it in the patient work environment. More precisely, we present an experiment that uses a smartphone from Google's Tango project and the Ovako Working Posture Analysing System ({OWAS}) method. To evaluate our approach, we also perform the ergonomic assessment using the Kinect sensor, a device that has a good reliability in the automated ergonomic evaluation. Both evaluations involved a set of 34 poses performed by 3 volunteers and annotated by an ergonomist. The Kinect has achieved accuracy of 57,08\% on torso classification, 58,33\% for arms and 25,00\% for legs positions. While the approach using the mobile depth sensor has achieved 35,41\% on torso classification, 93,05\% for arms and 66,23\% for legs positions on the same set of poses. Although the small sample, the achieved results may indicate that our mobile depth sensor approach can be as viable as methods based fixed depth sensor.},
	pages = {445--452},
	booktitle = {Proceedings of the 25th Brazillian Symposium on Multimedia and the Web},
	publisher = {{ACM}},
	author = {de Freitas, Pedro Vinicius A. and Mendes, Paulo Renato C. and Busson, Antonio José G. and Guedes, Alan Livio V. and Silva, Giovanni Lucca F. da and de Paiva, Anselmo Cardoso and Colcher, Sérgio},
	year = {2019}
}

@inproceedings{guedes_subjective_2019,
	title = {Subjective Evaluation of 360-degree Sensory Experiences},
	doi = {10.1109/MMSP.2019.8901743},
	abstract = {Traditionally, most multimedia content has been developed to stimulate two of the human senses, i.e., sight and hearing. Due to recent technological advancements, however, innovative services have been developed that provide more realistic, immersive, and engaging experiences to the audience. Omnidirectional (i.e., 360-degree) video, for instance, is becoming increasingly popular. It allows the viewer to navigate the full 360-degree view of a scene from a specific point. In particular, when consumed through head-mounted displays, 360-degree videos provide increased immersion and sense of presence. The use of multi-sensory effects —e.g., wind, vibration, and scent— has also been explored by recent work, which allows an improved experience by stimulating other users' senses through sensory effects that go beyond the audiovisual content. Understanding how these additional multi-sensory effects affect the users' perceived quality of experience ({QoE}) in 360-degree, however, is still an open research problem at large. As a step to better understand the {QoE} of immersive sensory experiences, this paper presents a testbed and discusses a user-focused study on a scenario in which the user is immersed in the 360-degree video content and is stimulated through additional sensory effects. Quantitative results indicated that the sensorial effects can considerably increase the sense of presence of 360-degree videos. Qualitative results provided us with a better view of the limitations of current technologies and interesting insights such as the users' sense of surprise.},
	eventtitle = {2019 {IEEE} 21st International Workshop on Multimedia Signal Processing ({MMSP})},
	pages = {1--6},
	booktitle = {2019 {IEEE} 21st International Workshop on Multimedia Signal Processing ({MMSP})},
	author = {Guedes, Alan L. V. and de A. Azevedo, Roberto G. and Frossard, Pascal and Colcher, Sérgio and Junqueira Barbosa, Simone Diniz},
	year = {2019},
	note = {{ISSN}: 2163-3517}
}

@inproceedings{kulesza_model-driven_2011,
	location = {Porto Alegre, Brazil, Brazil},
	title = {A Model-Driven Development Approach to Integration of Web Services and Interactive Applications: A Case Study in a Digital {TV} Platform},
	rights = {All rights reserved},
	url = {http://dl.acm.org/citation.cfm?id=3021508.3021524},
	series = {{WebMedia} 2011},
	shorttitle = {A Model-Driven Development Approach to Integration of Web Services and Interactive Applications},
	abstract = {This work proposes a model-driven development approach, which integrates interactive multimedia applications, and Web services. It is based on existing modeling language extended, which integrates modeling concepts for interactive applications and ads support for web services. Three Digital {TV} applications were modeled and developed. As we show, the evaluation of the approach brought benefits not supported by related works, like requirements structuring and reducing amount of work needed to finalize the code generated.},
	pages = {5:34--5:41},
	booktitle = {Proceedings of the 17th Brazilian Symposium on Multimedia and the Web on Brazilian Symposium on Multimedia and the Web},
	publisher = {Brazilian Computer Society},
	author = {Kulesza, Raoni and Meira, Silvio R.L. and Ferreira, Thales P. and Livio, Alan and Souza Filho, Guido L.S and Marques Neto, Manoel C. and Santos, Celso A.S.},
	year = {2011}
}

@inproceedings{kulesza_ginga-j:_2010,
	title = {Ginga-J: Ginga Middleware Imperative Environment Reference Implementation},
	rights = {All rights reserved},
	url = {http://portaldeconteudo.sbc.org.br/index.php/webmedia/article/download/5639/5536/},
	booktitle = {Proceedings of the 16th Brazilian Symposium on Multimedia and the Web},
	author = {Kulesza, Raoni and Lima, J. and Miranda Filho, S. and Guedes, Alan L. V. and Brandão, Rafael R. de M. and Araujo, Jônatas P. C. de and de Souza Filho, Guido L.},
	year = {2010}
}

@inproceedings{guedes_specification_2015,
	location = {New York, {NY}, {USA}},
	title = {Specification of Multimodal Interactions in {NCL}},
	rights = {All rights reserved},
	isbn = {978-1-4503-3959-9},
	url = {http://doi.acm.org/10.1145/2820426.2820436},
	doi = {10.1145/2820426.2820436},
	series = {{WebMedia} '15},
	abstract = {This paper proposes an approach to integrate multimodal events--both user-generated, e.g., audio recognizer, motion sensors; and user-consumed, e.g., speech synthesizer, haptic synthesizer--into programming languages for the declarative specification of multimedia applications. More precisely, it presents extensions to the {NCL} (Nested Context Language) multimedia language. {NCL} is the standard declarative language for the development of interactive applications for Brazilian Digital {TV} and an {ITU}-T Recommendation for {IPTV} services. {NCL} applications extended with the multimodal features are presented as results. Historically, Human-Computer Interaction research community has been focusing on user-generated modalities, through studies on the user interaction. On the other hand, Multimedia community has been focusing on output modalities, through studies on timing and multimedia processing. The proposals in this paper is an attempt to integrate concepts of both research communities in a unique high-level programming framework, which aims to assist the authoring of multimedia/multimodal applications.},
	pages = {181--187},
	booktitle = {Proceedings of the 21st Brazilian Symposium on Multimedia and the Web},
	publisher = {{ACM}},
	author = {Guedes, Alan L. V. and Azevedo, Roberto Gerson de Albuquerque and Moreno, Marcio Ferreira and Soares, Luiz Fernando Gomes},
	year = {2015}
}

@inproceedings{guedes_extending_2016,
	title = {Extending {NCL} to Support Multiuser and Multimodal Interactions},
	rights = {Copyright (c)},
	url = {https://sol.sbc.org.br/index.php/webmedia/article/view/5336},
	abstract = {Resumo
					Recent advances in technologies for speech, touch and gesture recognition have given rise to a new class of user interfaces that does not only explore multiple modalities but also allows for multiple interacting users. Even so, current declarative multimedia languages—e.g. {HTML}, {SMIL}, and {NCL}—support only limited forms of user input (mainly keyboard and mouse) for a single user. In this paper, we aim at studying how the {NCL} multimedia language could take advantage of those new recognition technologies. To do so, we revisit the model behind {NCL}, named {NCM} (Nested Context Model), and extend it with first-class concepts supporting multiuser and multimodal features. To evaluate our approach, we instantiate the proposal and discuss some usage scenarios, developed as {NCL} applications with our extended features.},
	pages = {39--46},
	booktitle = {Proceedings of the 22st Brazilian Symposium on Multimedia and the Web},
	author = {Guedes, Alan Livio Vasconcelos and Azevedo, Roberto Gerson De Albuquerque and Colcher, Sérgio and Barbosa, Simone D. J.},
	year = {2016},
	langid = {portuguese}
}

@inproceedings{gomes_soares_controlling_2015,
	location = {New York, {NY}, {USA}},
	title = {Controlling the Focus and Input Events in Multimedia Applications},
	rights = {All rights reserved},
	isbn = {978-1-4503-3196-8},
	url = {10.1145/2695664.2695885},
	doi = {http://doi.acm.org/10.1145/2695664.2695885},
	series = {{SAC} '15},
	abstract = {Multimedia applications can contain other embedded multimedia applications specified using the same or different languages. Therefore, during an application presentation there must be a mechanism to rule which language players and media players control commands (events) coming from input devices, and which ones control the focus on their related child objects, in a given moment of time. This paper presents a hierarchical control model addressing this mechanism. The proposal aims at supporting declarative languages targeting both Web and Digital {TV} applications. The model is first presented in its general scope. Then, its potential use by some standard declarative hypermedia languages ({HTML}-based, {SMIL} and {NCL}) is discussed. Since {NCL} has adopted the model in its standard specification, the discussion targeting {NCL} is more detailed, showing how the {NCL} player, part of the reference implementation of Ginga middleware, has been developed. {NCL} and Ginga are {ITU}-T H.761 Recommendation for {IPTV} services and part of the {ISDB}-T standards.},
	pages = {1278--1284},
	booktitle = {Proceedings of the 30th Annual {ACM} Symposium on Applied Computing},
	publisher = {{ACM}},
	author = {Gomes Soares, Luiz Fernando and Moreno, Marcio Ferreira and Guedes, Alan Alan L. V.},
	year = {2015}
}

@inproceedings{almeida_deep_2020,
	title = {A Deep Learning Approach to Detect Pornography Videos in Educational Repositories},
	volume = {30},
	pages = {912},
	booktitle = {Brazilian Symposium on Informatics in Education (Simpósio Brasileiro de Informática na Educação - {SBIE})},
	author = {Almeida, Pedro and Busson, Antônio and Guedes, Alan L. V. and Colcher, Sérgio},
	year = {2020},
	langid = {english}
}

@inproceedings{moraes_gamification_2019,
	title = {Gamification Analysis of a Virtual Learning Environment Based on Bartle Profiles},
	volume = {30},
	url = {https://www.br-ie.org/pub/index.php/sbie/article/view/8819},
	doi = {10.5753/cbie.sbie.2019.912},
	abstract = {Lack of engagement has been one of the main factors for evasion in higher education courses, especially among the exact sciences ones. In Computer Science courses, the scenario does not differ from the others, placing it among the highest evasion rates. Given the change in the teaching-learning process caused by the introduction of technologies, one of the alternatives to soften this problem is the use of elements of games, a technique known as gamification. This work presents an analysis of the use of gamification in a virtual learning environment ({VLE}) for the learning of algorithms. In particular, this gamification focus on specific players profiles, based on Bartle taxonomy, named as killers, socializers, explorers and achievers. We also performed a controlled experiment involving 44 algorithms students. As results, we present indications that the selected and applied gamification elements were promising in promoting the engagement and motivation of the users.},
	pages = {912},
	booktitle = {Brazilian Symposium on Informatics in Education (Simpósio Brasileiro de Informática na Educação - {SBIE})},
	author = {Moraes, Daniel de Sousa and Guedes, Alan and Castro, Vinícius Costa and Dias, Lukas Henrique and Neto, Carlos Soares},
	year = {2019},
	langid = {portuguese}
}

@inproceedings{mendes_authoring_2020,
	title = {An Authoring Model for Interactive 360 Videos},
	pages = {1--6},
	booktitle = {2020 {IEEE} International Conference on Multimedia \& Expo Workshops ({ICMEW})},
	publisher = {{IEEE}},
	author = {Mendes, Paulo {RC} and Guedes, Alan {LV} and Moraes, Daniel de S. and Azevedo, Roberto {GA} and Colcher, Sérgio},
	year = {2020}
}

@inproceedings{busson_video_2020,
	title = {Video Quality Enhancement Using Deep Learning-Based Prediction Models for Quantized {DCT} Coefficients in {MPEG} I-frames},
	booktitle = {2020 {IEEE} International Symposium on Multimedia ({ISM})},
	author = {Busson, Antonio {JG} and Mendes, Paulo {RC} and Moraes, Daniel de S and da Veiga, Álvaro M and Guedes, Alan {LV} and Colcher, Sérgio},
	year = {2020}
}

@inproceedings{mendes_clustering-based_2020,
	title = {A Clustering-Based Method for Automatic Educational Video Recommendation Using Deep Face-Features of Lecturers},
	booktitle = {2020 {IEEE} International Symposium on Multimedia ({ISM})},
	author = {Mendes, Paulo {RC} and Vieira, Eduardo S and Guedes, Alan {LV} and Busson, Antonio {JG} and Colcher, Sérgio},
	year = {2020}
}